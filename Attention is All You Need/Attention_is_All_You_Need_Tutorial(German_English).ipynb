{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ScHfAKFRghm0","outputId":"191849cf-02ae-41c0-a2c2-7232f61fd463","executionInfo":{"status":"ok","timestamp":1668151287914,"user_tz":480,"elapsed":1205,"user":{"displayName":"Jihyeok Choi","userId":"13786222269413320027"}}},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WS7er2RNh0wW","executionInfo":{"status":"ok","timestamp":1668151417362,"user_tz":480,"elapsed":3,"user":{"displayName":"Jihyeok Choi","userId":"13786222269413320027"}},"outputId":"a6921725-45b8-4b2d-c177-4a48286af85d"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}]},{"cell_type":"code","source":["!git clone https://saurus2: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@github.com/saurus2/NLP.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oZbyCN78iUdx","executionInfo":{"status":"ok","timestamp":1668151424370,"user_tz":480,"elapsed":1096,"user":{"displayName":"Jihyeok Choi","userId":"13786222269413320027"}},"outputId":"8c948fd0-79f3-45ea-f0af-1bf07f6300c7"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'NLP'...\n","remote: Enumerating objects: 3, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (3/3), done.\n"]}]},{"cell_type":"markdown","source":["# Attention is All You Need(NIPS 2017)\n","* This code refered Transformer Paper\n"],"metadata":{"id":"KrQnXNESMr-k"}},{"cell_type":"markdown","source":["## BLEU Score Calculation Library\n","### Bilingual Evaluation Understudy Score\n","BLEU compare translation between human's and machine learning task.  \n","The standard of measurement is baed on n-gram.  "],"metadata":{"id":"urUWZRvfM7Lx"}},{"cell_type":"code","source":["!pip install torchtext==0.6.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VR_8gN0MM5iX","outputId":"e6193981-8a9c-42a7-b95a-6b208edc9206"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchtext==0.6.0\n","  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n","\u001b[K     |████████████████████████████████| 64 kB 2.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.21.6)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.12.1+cu113)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 38.5 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.64.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (4.1.1)\n","Installing collected packages: sentencepiece, torchtext\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.13.1\n","    Uninstalling torchtext-0.13.1:\n","      Successfully uninstalled torchtext-0.13.1\n","Successfully installed sentencepiece-0.1.97 torchtext-0.6.0\n"]}]},{"cell_type":"markdown","source":["## Data Preprocessing\n","* spaCy: This library is for tokenization and tagging and so on for data preprocessing\n","* Install preprocessing module for english and Deutsch"],"metadata":{"id":"FuVrLPy1N40M"}},{"cell_type":"code","source":["%%capture\n","!python -m spacy download en_core_web_sm\n","!python -m spacy download nl_core_news_sm"],"metadata":{"id":"VjgHbR1rNBxc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assign objects for english and Deutsch\n","!pip install spacy\n","import spacy\n","\n","spacy_en = spacy.load(\"en_core_web_sm\") # English tokenization\n","spacy_de = spacy.load(\"nl_core_news_sm\") # Deutsch tokenization"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FwvBka5_OWRy","outputId":"da2f5483-51aa-4cfb-cba6-984d477c61fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.9)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.5)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.10.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.10.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"]}]},{"cell_type":"code","source":["# Use simple example of tokenization\n","tokenized = spacy_en.tokenizer(\"I am a graduate studnet.\")\n","\n","for i, token in enumerate(tokenized):\n","  print(f'Index {i}: {token.text}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TaXyA3CdOm5s","outputId":"fd9cfcbc-c736-41f6-ff30-f9a19541e000"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Index 0: I\n","Index 1: am\n","Index 2: a\n","Index 3: graduate\n","Index 4: studnet\n","Index 5: .\n"]}]},{"cell_type":"markdown","source":["### Define functions for English and Deutsch"],"metadata":{"id":"-fN0T-BmQTOs"}},{"cell_type":"code","source":["# Deutschs Tokenizer function\n","def tokenize_de(text):\n","  return [token.text for token in spacy_de.tokenizer(text)]"],"metadata":{"id":"gdBJ8bW5QMWx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# English Tokenizer function\n","def tokenize_en(text):\n","  return [token.text for token in spacy_en.tokenizer(text)]"],"metadata":{"id":"b67vDjoVQn4p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Notice preprocessing detail for dataset by using field library\n","Source: Deutsch  \n","Traget: English"],"metadata":{"id":"2V4SKzgYQxi5"}},{"cell_type":"code","source":["from torchtext.data import Field, BucketIterator"],"metadata":{"id":"PxU5IfQkQtey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SRC = Field(tokenize=tokenize_de, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, batch_first=True)\n","TRG = Field(tokenize=tokenize_en, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, batch_first=True)"],"metadata":{"id":"ZRiS8WDTRN79"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load Multi30k for english-Deutsch"],"metadata":{"id":"hdAT4gD1Rkm_"}},{"cell_type":"code","source":["from torchtext.datasets import Multi30k\n","train_dataset, valid_dataset, test_dataset = Multi30k.splits(exts=(\".de\", \".en\"), fields=(SRC, TRG))"],"metadata":{"id":"ZMyuQFgZRjJc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'Train dataset size: {len(train_dataset.examples)}')\n","print(f'Validation dataset size: {len(valid_dataset)}')\n","print(f'Testing dataset size: {len(test_dataset)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gNUpS1PNR2Q-","outputId":"ed485457-a256-45ba-9b74-3eee3847a01d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train dataset size: 29000\n","Validation dataset size: 1014\n","Testing dataset size: 1000\n"]}]},{"cell_type":"code","source":["# Print one of the train_datasets\n","print(vars(train_dataset.examples[30])['src'])\n","print(vars(train_dataset.examples[30])['trg'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NjeQAEdwSJfI","outputId":"b68f8e9c-ccaa-42c2-bb4c-db50675dd8bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['ein', 'mann', ',', 'der', 'mit', 'einer', 'tasse', 'kaffee', 'an', 'einem', 'urinal', 'steht', '.']\n","['a', 'man', 'standing', 'at', 'a', 'urinal', 'with', 'a', 'coffee', 'cup', '.']\n"]}]},{"cell_type":"markdown","source":["### The object of field method will create dictionaries for English and Deutsch\n","Select words showing on twice time only"],"metadata":{"id":"Ia13CJysSnRN"}},{"cell_type":"code","source":["SRC.build_vocab(train_dataset, min_freq=2)\n","TRG.build_vocab(train_dataset, min_freq=2)\n","\n","print(f'len(src): {len(SRC.vocab)}')\n","print(f'len(trg): {len(TRG.vocab)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kfpiEatWShkQ","outputId":"e0e746fb-8777-431b-a3f4-6c89a0ec9a65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["len(src): 7853\n","len(trg): 5893\n"]}]},{"cell_type":"code","source":["# Show the location for word with index\n","print(TRG.vocab.stoi['abcabc']) # not exist word: 0\n","print(TRG.vocab.stoi[TRG.pad_token]) # padding: 1\n","print(TRG.vocab.stoi['<sos>']) # <sos>: 2\n","print(TRG.vocab.stoi['<eos>']) # <eos>: 3\n","print(TRG.vocab.stoi['hello'])\n","print(TRG.vocab.stoi['world'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-3tTuPnTDDI","outputId":"2539d05e-9f1e-4327-a501-8c7f3fe4daa0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","2\n","3\n","4112\n","1752\n"]}]},{"cell_type":"markdown","source":["### Words has to put in a network orderly\n","* It's better to match the number of words\n","* Then, use BucketIterator\n","* Batch size: 128"],"metadata":{"id":"xBGfXA5vTogy"}},{"cell_type":"code","source":["import torch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"190H64FuTmZv","outputId":"a7c8e74d-d931-4634-cf60-f2d126fce975"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["# Reduce dementionality by using less batch size\n","BATCH_SIZE = 128\n","\n","# It is able to use common data loader\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_dataset, valid_dataset, test_dataset),\n","    batch_size=BATCH_SIZE,\n","    device=device\n",")"],"metadata":{"id":"HnSal6TcUJVK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check batch for examples\n","for i, batch in enumerate(train_iterator):\n","  src = batch.src\n","  trg = batch.trg\n","  \n","  print(f'The size of the first batch: {src.shape}')\n","  \n","  # Print inoformation in current batch\n","  for i in range(src.shape[1]):\n","    print(f'index {i}: {src[0][i].item()}') # [Seq_num, Seq_len]\n","  \n","  # Check the first batch then done\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JYTTJRTWUfCh","outputId":"7bd635af-bd0a-4d1e-fd30-703eb1067d65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The size of the first batch: torch.Size([128, 28])\n","index 0: 2\n","index 1: 5\n","index 2: 821\n","index 3: 32\n","index 4: 1414\n","index 5: 23\n","index 6: 20\n","index 7: 118\n","index 8: 140\n","index 9: 4\n","index 10: 3\n","index 11: 1\n","index 12: 1\n","index 13: 1\n","index 14: 1\n","index 15: 1\n","index 16: 1\n","index 17: 1\n","index 18: 1\n","index 19: 1\n","index 20: 1\n","index 21: 1\n","index 22: 1\n","index 23: 1\n","index 24: 1\n","index 25: 1\n","index 26: 1\n","index 27: 1\n"]}]},{"cell_type":"markdown","source":["### Multi Head Attention Architecture\n","* Attention needs three elements\n","  * Queries\n","  * Keys\n","  * Values\n","  * Query, Key, Value are on same dimentionality\n","* Hyperparameter\n","  * hidden_dim: Embedding dimention for one word\n","  * n_heads: The number of head = Scaled dot-product attention\n","  * dropout_ratio: The ratio of dropout"],"metadata":{"id":"Uf6ssU8PZsD7"}},{"cell_type":"code","source":["import torch.nn as nn"],"metadata":{"id":"a-ulamG_VZmC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a class of multi head attention layer\n","class MultiHeadAttentionLayer(nn.Module):\n","  def __init__(self, hidden_dim, n_heads, dropout_ratio, device):\n","    super().__init__()\n","\n","    assert hidden_dim % n_heads == 0\n","\n","    self.hidden_dim = hidden_dim # Embedding demention\n","    self.n_heads = n_heads # The number of head: The number of concept from each different attentions\n","    self.head_dim = hidden_dim // n_heads\n","\n","    self.fc_q = nn.Linear(hidden_dim, hidden_dim) # FC layer for applying Query\n","    self.fc_k = nn.Linear(hidden_dim, hidden_dim) # FC layer for applying Key\n","    self.fc_v = nn.Linear(hidden_dim, hidden_dim) # FC layer for applying Value\n","\n","    self.fc_o = nn.Linear(hidden_dim, hidden_dim)\n","\n","    self.dropout = nn.Dropout(dropout_ratio)\n","\n","    self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n","  # Define a forward function\n","  def forward(self, query, key, value, mask = None):\n","    batch_size = query.shape[0]\n","\n","    # query: [batch_size, query_len, hidden_dim]\n","    # key: [batch_size, key_len, hidden_dim]\n","    # value: [batch_size, value_len, hidden_dim]\n","\n","    Q = self.fc_q(query)\n","    K = self.fc_k(key)\n","    V = self.fc_v(value)\n","\n","    # Q: [batch_size, query_len, hidden_dim]\n","    # K: [batch_size, key_len, hidden_dim]\n","    # V: [batch_size, value_len, hidden_dim]\n","\n","    # hidden_dim -> n_heads * head_dim\n","    # Intend to train each attention for the number of n_head(h)\n","    Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","    K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","    V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","\n","    # Q: [batch_size, n_heads, query_len, head_dim]\n","    # K: [batch_size, n_heads, key_len, head_dim]\n","    # V: [batch_size, n_heads, value_len, head_dim]\n","\n","    # Calculate Attention Energy\n","    energy = torch.matmul(0, K.permute(0, 1, 3, 2)) / self.scale\n","\n","    # energy: [batch_size, n_heads, query_len, key_len]\n","\n","    # The case for using mask\n","    if mask is not None:\n","      # Fill out -1e10 for mask value being 0\n","      energy = energy.masked_fill(mask==0, -1e10)\n","    \n","    # The probability of each words. \n","    # It is Attention score calculation\n","    attention = torch.softmax(energy, dim=-1)\n","\n","    # attention: [batch_size, n_heads, query_len, key_len]\n","\n","    # Calculate Scaled Dot-Product Attention\n","    x = torch.matmul(self.dropout(attention), V)\n","\n","    # x: [batch_size, n_heads, query_len, head_dim]\n","\n","    x = x.permute(0, 2, 1, 3).contiguous()\n","\n","    # x: [batch_size, query_len, n_heads, head_dim]\n","    x = x.view(batch_size, -1, self.hidden_dim)\n","\n","    # x: [batch_size, query_len, hidden_dim]\n","\n","    x = self.fc_o(x)\n","\n","    # x: [batch_size, query_len, hidden_dim]\n","    return x, attention\n","\n"],"metadata":{"id":"lAPP7aZYbkmY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git config --global user.email 'bluesaurus2@gmail.com'\n","!git config --global user.name 'saurus2'"],"metadata":{"id":"oqYO6yCEgfVu","executionInfo":{"status":"ok","timestamp":1668151661857,"user_tz":480,"elapsed":479,"user":{"displayName":"Jihyeok Choi","userId":"13786222269413320027"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["!git add ."],"metadata":{"id":"wO2BL1aYdICb","executionInfo":{"status":"ok","timestamp":1668151663597,"user_tz":480,"elapsed":578,"user":{"displayName":"Jihyeok Choi","userId":"13786222269413320027"}}},"execution_count":42,"outputs":[]}]}